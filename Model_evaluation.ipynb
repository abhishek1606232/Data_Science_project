{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name, plot_curves=True):\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test, batch_size=32, verbose=1)\n",
        "    y_pred_binary = (y_pred > 0.5).astype(np.float32)\n",
        "\n",
        "    # Flatten for sklearn metrics\n",
        "    y_true_flat = y_test.reshape(-1).astype(np.int32)\n",
        "    y_pred_flat = y_pred_binary.reshape(-1).astype(np.int32)\n",
        "    y_pred_prob_flat = y_pred.reshape(-1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "    recall = recall_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "    f1 = f1_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "    iou = jaccard_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"IoU: {iou:.4f}\")\n",
        "\n",
        "    # Calculate dice score\n",
        "    dice = dice_coefficient(\n",
        "        tf.convert_to_tensor(y_test),\n",
        "        tf.convert_to_tensor(y_pred_binary)\n",
        "    )\n",
        "    print(f\"Dice Coefficient: {dice:.4f}\")\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true_flat, y_pred_prob_flat)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(y_true_flat, y_pred_prob_flat)\n",
        "    pr_auc = auc(recall_curve, precision_curve)\n",
        "\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "    # Plot curves if requested\n",
        "    if plot_curves:\n",
        "        plot_single_model_curves(y_true_flat, y_pred_prob_flat, model_name, fpr, tpr, roc_auc, recall_curve, precision_curve, pr_auc)\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'iou': iou,\n",
        "        'dice': float(dice),\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_binary': y_pred_binary,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'precision_curve': precision_curve,\n",
        "        'recall_curve': recall_curve\n",
        "    }\n",
        "\n",
        "def plot_single_model_curves(y_true, y_pred_prob, model_name, fpr, tpr, roc_auc, recall_curve, precision_curve, pr_auc):\n",
        "    \"\"\"Plot ROC and PR curves for a single model\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # ROC Curve\n",
        "    axes[0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "    axes[0].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "    axes[0].set_xlim([0.0, 1.0])\n",
        "    axes[0].set_ylim([0.0, 1.05])\n",
        "    axes[0].set_xlabel('False Positive Rate')\n",
        "    axes[0].set_ylabel('True Positive Rate')\n",
        "    axes[0].set_title(f'{model_name} - ROC Curve')\n",
        "    axes[0].legend(loc=\"lower right\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    axes[1].plot(recall_curve, precision_curve, color='red', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
        "    axes[1].set_xlim([0.0, 1.0])\n",
        "    axes[1].set_ylim([0.0, 1.05])\n",
        "    axes[1].set_xlabel('Recall')\n",
        "    axes[1].set_ylabel('Precision')\n",
        "    axes[1].set_title(f'{model_name} - Precision-Recall Curve')\n",
        "    axes[1].legend(loc=\"lower left\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curves(all_metrics, y_test):\n",
        "    \"\"\"Plot ROC curves for multiple models\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for metrics in all_metrics:\n",
        "        y_true_flat = y_test.reshape(-1).astype(np.int32)\n",
        "        y_pred_prob_flat = metrics['y_pred'].reshape(-1)\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true_flat, y_pred_prob_flat)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, label=f\"{metrics['model_name']} (AUC = {roc_auc:.3f})\", linewidth=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, linewidth=1)\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curves - Model Comparison', fontsize=14)\n",
        "    plt.legend(loc='lower right', fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def plot_pr_curves(all_metrics, y_test):\n",
        "    \"\"\"Plot Precision-Recall curves for multiple models\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for metrics in all_metrics:\n",
        "        y_true_flat = y_test.reshape(-1).astype(np.int32)\n",
        "        y_pred_prob_flat = metrics['y_pred'].reshape(-1)\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(y_true_flat, y_pred_prob_flat)\n",
        "        pr_auc = auc(recall, precision)\n",
        "\n",
        "        plt.plot(recall, precision, label=f\"{metrics['model_name']} (AUC = {pr_auc:.3f})\", linewidth=2)\n",
        "\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title('Precision-Recall Curves - Model Comparison', fontsize=14)\n",
        "    plt.legend(loc='lower left', fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WYPe11mPMCTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}